{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy import stats\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data111.xlsx', sheet_name='prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the font family to Times New Roman for all text elements\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "year_count = df.groupby('Year')['Year'].count()\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(year_count.index, year_count.values)\n",
    "\n",
    "# set the x-ticks to show each year\n",
    "plt.xticks(year_count.index)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of papers')\n",
    "plt.title('Number of Worked papers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "year_count = df.groupby('Year')['Year'].count()\n",
    "# Convert the year index to integers and reindex to include missing years\n",
    "year_count = year_count.astype(int).reindex(range(2005, 2023), fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Normalize the y-values between 0 and 1\n",
    "norm = mcolors.Normalize(vmin=year_count.min(), vmax=year_count.max())\n",
    "\n",
    "# Define the colormap\n",
    "cmap = cm.get_cmap('Blues')\n",
    "\n",
    "# Plot the bars with varying colors based on y-values\n",
    "bars = plt.bar(year_count.index, year_count.values, color=cmap(norm(year_count.values)))\n",
    "\n",
    "plt.xticks(year_count.index)\n",
    "plt.xlabel('Year')\n",
    "\n",
    "# Set the y-axis limit to start from 0\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "# Use math.floor() to convert the y-axis tick labels to integers\n",
    "plt.yticks([math.floor(y) for y in plt.yticks()[0]])\n",
    "plt.ylabel('Number of papers')\n",
    "plt.title('Number of Worked papers by year')\n",
    "\n",
    "# Adjust the alpha (transparency) of the bars for better visibility\n",
    "for bar in bars:\n",
    "    bar.set_alpha(0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_count = df.groupby((df['Year']//10)*10)['Year'].count()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# create the second bar chart by decade\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(decade_count.index, decade_count.values)\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Number of papers')\n",
    "plt.title('Number of Worked papers by decade')\n",
    "\n",
    "# set the x-ticks to show only 2000s, 2010s, and 2020s\n",
    "plt.xticks([2000, 2010, 2020])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the \"Used Techniques\" column by \"-\" separator and create a list of all techniques\n",
    "technique_df = df['Used Techniques'].str.split('-', expand=True)\n",
    "technique_series = technique_df.stack().reset_index(drop=True)\n",
    "technique_counts = technique_series.value_counts()\n",
    "\n",
    "other_techniques = []\n",
    "for i in range(len(technique_counts)):\n",
    "    if technique_counts[i] == 1:\n",
    "        other_techniques.append(technique_counts.index[i])\n",
    "\n",
    "# Replace techniques mentioned only once with \"Other Techniques\"\n",
    "other_techniques_count = sum([count for count in technique_counts if count <= 1])\n",
    "technique_counts = technique_counts[technique_counts > 1]\n",
    "technique_counts['Other Techniques'] = other_techniques_count\n",
    "\n",
    "# Create the figure and axes\n",
    "pie, ax = plt.subplots(figsize=[8, 8])\n",
    "\n",
    "# Create the pie chart\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    technique_counts.values,\n",
    "    labels=technique_counts.index,\n",
    "    autopct='%1.1f%%', labeldistance=0.80\n",
    ")\n",
    "\n",
    "# Customize the text labels inside the pie chart\n",
    "for text in texts:\n",
    "    text.set_horizontalalignment('center')\n",
    "    text.set_fontname('Times New Roman')\n",
    "\n",
    "# Customize the percent labels\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontstyle('italic')\n",
    "    autotext.set_fontname('Times New Roman')\n",
    "    autotext.set_fontsize(14)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# Plot the legend\n",
    "ax.legend(wedges, other_techniques, title=\"Other Techniques:\", loc=\"best\", bbox_to_anchor=(1, 0, 0.5, 1), handlelength=0)\n",
    "\n",
    "plt.title('Used Techniques', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "# Split the \"Field of Data\" column by \"-\" separator and create a list of all fields\n",
    "fields_df = df['Field of Data'].str.split('-', expand=True)\n",
    "fields_series = fields_df.stack().reset_index(drop=True)\n",
    "fields_counts = fields_series.value_counts()\n",
    "\n",
    "other_fields = []\n",
    "for i in range(len(fields_counts)):\n",
    "    if fields_counts[i] == 1:\n",
    "        other_fields.append(fields_counts.index[i])\n",
    "\n",
    "\n",
    "# Replace fields mentioned only once with \"Other Fields\"\n",
    "other_fields_count = sum([count for count in fields_counts if count <= 1])\n",
    "fields_counts = fields_counts[fields_counts > 1]\n",
    "fields_counts['Other Fields'] = other_fields_count\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "plt.pie(fields_counts.values, labels=fields_counts.index, autopct='%1.1f%%')\n",
    "\n",
    "ax.legend(wedges, other_fields, title=\"Other Fields:\", loc=\"best\", bbox_to_anchor=(1, 0, 0.5, 1), handlelength=0)\n",
    "\n",
    "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "fig = plt.gcf()\n",
    " \n",
    "# Adding Circle in Pie chart\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.title('Field of Data', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression analysis\n",
    "\n",
    "df2 = pd.read_excel('data111.xlsx', sheet_name='data')\n",
    "data = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictor variables\n",
    "variables = ['SH', 'SUH', 'EH', 'AD', 'BPL', 'PH', 'KH', 'TH', 'HW']\n",
    "y1 = data[['SH']]\n",
    "y2 = data[['SUH']]\n",
    "y3 = data[['EH']]\n",
    "y4 = data[['AD']]\n",
    "y5 = data[['BPL']]\n",
    "y6 = data[['PH']]\n",
    "y7 = data[['KH']]\n",
    "y8 = data[['TH']]\n",
    "y9 = data[['HW']]\n",
    "# Define dependent variable\n",
    "X = data['S']\n",
    "# Fit the linear regression model\n",
    "model1 = sm.OLS(y1, sm.add_constant(X)).fit()\n",
    "model2 = sm.OLS(y2, sm.add_constant(X)).fit()\n",
    "model3 = sm.OLS(y3, sm.add_constant(X)).fit()\n",
    "model4 = sm.OLS(y4, sm.add_constant(X)).fit()\n",
    "model5 = sm.OLS(y5, sm.add_constant(X)).fit()\n",
    "model6 = sm.OLS(y6, sm.add_constant(X)).fit()\n",
    "model7 = sm.OLS(y7, sm.add_constant(X)).fit()\n",
    "model8 = sm.OLS(y8, sm.add_constant(X)).fit()\n",
    "model9 = sm.OLS(y9, sm.add_constant(X)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print linear equation\n",
    "print('Shoulder Height = {:.3f} + {:.3f} * Stature'.format (model1.params['const'], model1.params['S']))\n",
    "# Print R-squared and adjusted R-squared\n",
    "print('R-squared: {:.3f}'.format(model1.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model1.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model1.bse['const']))\n",
    "\n",
    "\n",
    "print('Linear regression:')\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(X, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Reshape the data to a 2D array\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y1_train = y1_train.values.reshape(-1, 1)\n",
    "y1_test = y1_test.values.reshape(-1, 1)\n",
    "\n",
    "# Fit the linear regression model on the training data\n",
    "reg1 = LinearRegression().fit(X_train, y1_train)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print('Coefficients:', reg1.coef_)\n",
    "print('Intercept:', reg1.intercept_)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y1_pred = reg1.predict(X_test)\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r_squared = reg1.score(X_test, y1_test)\n",
    "print('R-squared:', r_squared)\n",
    "# Calculate the adjusted R-squared value\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subscapular Height = {:.3f} + {:.3f} * Stature'.format (model2.params['const'], model2.params['S']))\n",
    "print('R-squared: {:.3f}'.format(model2.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model2.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model2.bse['const']))\n",
    "print('Linear regression:')\n",
    "X_train, X_test, y2_train, y2_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y2_train = y2_train.values.reshape(-1, 1)\n",
    "y2_test = y2_test.values.reshape(-1, 1)\n",
    "\n",
    "reg2 = LinearRegression().fit(X_train, y2_train)\n",
    "\n",
    "print('Coefficients:', reg2.coef_)\n",
    "print('Intercept:', reg2.intercept_)\n",
    "\n",
    "y2_pred = reg2.predict(X_test)\n",
    "\n",
    "r_squared = reg2.score(X_test, y2_test)\n",
    "print('R-squared:', r_squared)\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Elbow Height = {:.3f} + {:.3f} * Stature'.format (model3.params['const'], model3.params['S']))\n",
    "print('R-squared: {:.3f}'.format(model3.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model3.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model3.bse['const']))\n",
    "print('Linear regression:')\n",
    "\n",
    "X_train, X_test, y3_train, y3_test = train_test_split(X, y3, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y3_train = y3_train.values.reshape(-1, 1)\n",
    "y3_test = y3_test.values.reshape(-1, 1)\n",
    "\n",
    "reg3 = LinearRegression().fit(X_train, y3_train)\n",
    "\n",
    "print('Coefficients:', reg3.coef_)\n",
    "print('Intercept:', reg3.intercept_)\n",
    "\n",
    "y3_pred = reg3.predict(X_test)\n",
    "\n",
    "r_squared = reg3.score(X_test, y3_test)\n",
    "print('R-squared:', r_squared)\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Abdominal Depth = {:.3f} + {:.3f} * Stature'.format (model4.params['const'], model4.params['S']))\n",
    "print('R-squared: {:.3f}'.format(model4.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model4.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model4.bse['const']))\n",
    "print('Linear regression:')\n",
    "\n",
    "X_train, X_test, y4_train, y4_test = train_test_split(X, y4, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y4_train = y4_train.values.reshape(-1, 1)\n",
    "y4_test = y4_test.values.reshape(-1, 1)\n",
    "\n",
    "reg4 = LinearRegression().fit(X_train, y4_train)\n",
    "\n",
    "print('Coefficients:', reg4.coef_)\n",
    "print('Intercept:', reg4.intercept_)\n",
    "\n",
    "y4_pred = reg4.predict(X_test)\n",
    "\n",
    "r_squared = reg4.score(X_test, y4_test)\n",
    "print('R-squared:', r_squared)\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Buttock Popliteal Length = {:.3f} + {:.3f} * Stature'.format (model5.params['const'], model5.params['S']))\n",
    "print('R-squared: {:.3f}'.format(model5.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model5.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model5.bse['const']))\n",
    "print('Linear regression:')\n",
    "\n",
    "X_train, X_test, y5_train, y5_test = train_test_split(X, y5, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y5_train = y5_train.values.reshape(-1, 1)\n",
    "y5_test = y5_test.values.reshape(-1, 1)\n",
    "\n",
    "reg5 = LinearRegression().fit(X_train, y5_train)\n",
    "\n",
    "print('Coefficients:', reg5.coef_)\n",
    "print('Intercept:', reg5.intercept_)\n",
    "\n",
    "y5_pred = reg5.predict(X_test)\n",
    "\n",
    "r_squared = reg5.score(X_test, y5_test)\n",
    "print('R-squared:', r_squared)\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Popliteal Height = {:.3f} + {:.3f} * Stature'.format (model6.params['const'], model6.params['S']))\n",
    "print('R-squared: {:.3f}'.format(model6.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model6.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model6.bse['const']))\n",
    "print('Linear regression:')\n",
    "\n",
    "X_train, X_test, y6_train, y6_test = train_test_split(X, y6, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y6_train = y6_train.values.reshape(-1, 1)\n",
    "y6_test = y6_test.values.reshape(-1, 1)\n",
    "\n",
    "reg6 = LinearRegression().fit(X_train, y6_train)\n",
    "\n",
    "print('Coefficients:', reg6.coef_)\n",
    "print('Intercept:', reg6.intercept_)\n",
    "\n",
    "y6_pred = reg6.predict(X_test)\n",
    "\n",
    "r_squared = reg6.score(X_test, y6_test)\n",
    "print('R-squared:', r_squared)\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('knee Height = {:.3f} + {:.3f} * Stature'.format (model7.params['const'], model7.params['S']))\n",
    "print('R-squared: {:.3f}'.format(model7.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model7.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model7.bse['const']))\n",
    "print('Linear regression:')\n",
    "\n",
    "X_train, X_test, y7_train, y7_test = train_test_split(X, y7, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y7_train = y7_train.values.reshape(-1, 1)\n",
    "y7_test = y7_test.values.reshape(-1, 1)\n",
    "\n",
    "reg7 = LinearRegression().fit(X_train, y7_train)\n",
    "\n",
    "print('Coefficients:', reg7.coef_)\n",
    "print('Intercept:', reg7.intercept_)\n",
    "\n",
    "y7_pred = reg7.predict(X_test)\n",
    "\n",
    "r_squared = reg7.score(X_test, y7_test)\n",
    "print('R-squared:', r_squared)\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Thigh Thickness = {:.3f} + {:.3f} * Stature'.format (model8.params['const'], model8.params['S']))\n",
    "print('R-squared: {:.3f}'.format(model8.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model8.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model8.bse['const']))\n",
    "print('Linear regression:')\n",
    "\n",
    "X_train, X_test, y8_train, y8_test = train_test_split(X, y8, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y8_train = y8_train.values.reshape(-1, 1)\n",
    "y8_test = y8_test.values.reshape(-1, 1)\n",
    "\n",
    "reg8 = LinearRegression().fit(X_train, y8_train)\n",
    "\n",
    "print('Coefficients:', reg8.coef_)\n",
    "print('Intercept:', reg8.intercept_)\n",
    "\n",
    "y8_pred = reg8.predict(X_test)\n",
    "\n",
    "r_squared = reg8.score(X_test, y8_test)\n",
    "print('R-squared:', r_squared)\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hip Width = {:.3f} + {:.3f} * Stature'.format (model9.params['const'], model9.params['S']))\n",
    "print('R-squared: {:.3f}'.format(model9.rsquared))\n",
    "print('Adj. R-squared: {:.3f}'.format(model9.rsquared_adj))\n",
    "print('Standard error of the regression: {:.3f}'.format(model9.bse['const']))\n",
    "print('Linear regression:')\n",
    "\n",
    "X_train, X_test, y9_train, y9_test = train_test_split(X, y9, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.values.reshape(-1, 1)\n",
    "X_test = X_test.values.reshape(-1, 1)\n",
    "y9_train = y9_train.values.reshape(-1, 1)\n",
    "y9_test = y9_test.values.reshape(-1, 1)\n",
    "\n",
    "reg9 = LinearRegression().fit(X_train, y9_train)\n",
    "\n",
    "print('Coefficients:', reg9.coef_)\n",
    "print('Intercept:', reg9.intercept_)\n",
    "\n",
    "y9_pred = reg9.predict(X_test)\n",
    "\n",
    "r_squared = reg9.score(X_test, y9_test)\n",
    "print('R-squared:', r_squared)\n",
    "n = len(X_test)\n",
    "p = X_train.shape[1]\n",
    "adj_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print('Adjusted R-squared:', adj_r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Define the predictor and dependent variables\n",
    "y = data[['SH', 'SUH', 'EH', 'AD', 'BPL', 'PH', 'KH', 'TH', 'HW']]\n",
    "X = data['S']\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(16,16))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Create scatter plot with regression line for each feature\n",
    "for i, feature in enumerate(y.columns):\n",
    "    model = sm.OLS(y[feature], sm.add_constant(X)).fit()\n",
    "    y_pred = model.predict(sm.add_constant(X))\n",
    "    r2_adj = model.rsquared_adj.round(3)\n",
    "    eq = f'y = {model.params[0].round(3)} + {model.params[1].round(3)}x'\n",
    "    sns.regplot(x=X, y=feature, data=data, ax=axs[i], scatter_kws={'alpha':0.2})\n",
    "    axs[i].set_xlabel('Stature')\n",
    "    axs[i].set_ylabel(feature)\n",
    "    axs[i].text(0.05, 0.9, eq, transform=axs[i].transAxes)\n",
    "    axs[i].text(0.05, 0.8, f'Adj R2: {r2_adj}', transform=axs[i].transAxes)\n",
    "\n",
    "fig.suptitle('Scatter plot of Stature and Anthopometry metrics', fontsize=20)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering\n",
    "\n",
    "df4 = pd.read_excel('data111.xlsx', sheet_name='data')\n",
    "df4 = df4.dropna()\n",
    "\n",
    "# encode 'Sex' and 'Degree' columns\n",
    "le = LabelEncoder()\n",
    "df4['Sex'] = le.fit_transform(df4['Sex'])\n",
    "degree_map = {'Bachelor': 1, 'Master': 2, 'Phd': 3}\n",
    "df4['Degree'] = df4['Degree'].map(degree_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# select the features to cluster on\n",
    "features = ['S', 'SH', 'SUH', 'EH', 'AD', 'BPL', 'PH', 'KH', 'TH', 'HW', 'Age', 'Weight']\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df4[features])\n",
    "\n",
    "# within-cluster sum of squares (WCSS)\n",
    "wcss = []\n",
    "for i in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the WCSS\n",
    "plt.plot(range(1, 10), wcss, marker='o', linestyle='-', color='b')\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set the range of k values to test\n",
    "k_values = [2, 3, 4]\n",
    "\n",
    "# Create a figure and subplots with a 1x3 grid layout\n",
    "fig, axes = plt.subplots(1, len(k_values), figsize=(12, 4))\n",
    "\n",
    "# Plot scatter plots for each k value\n",
    "for i, k in enumerate(k_values):\n",
    "    # Fit k-means model\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    \n",
    "    # Get cluster labels and distances\n",
    "    labels = kmeans.labels_\n",
    "    distances = pairwise_distances(scaled_data, kmeans.cluster_centers_)\n",
    "    avg_distances = np.mean(np.min(distances, axis=1))\n",
    "\n",
    "    # Plot scatter plot with different colors for each cluster\n",
    "    axes[i].scatter(scaled_data[:, 0], scaled_data[:, 1], c=labels, cmap='viridis')\n",
    "    axes[i].set_title(f'k = {k}, Avg Distance = {avg_distances:.2f}')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the combined plot as a single image\n",
    "plt.savefig('combined_scatter_plots.png')\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
    "kmeans.fit(scaled_data)\n",
    "\n",
    "\n",
    "# apply hierarchical clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "hierarchical.fit(scaled_data)\n",
    "\n",
    "# add the cluster labels to the original dataframe and save it to a new file\n",
    "df4['kmeans_cluster'] = kmeans.labels_\n",
    "df4['hierarchical_cluster'] = hierarchical.labels_\n",
    "data = df4.to_csv('clustered_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('clustered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot of each variable\n",
    "#with each box representing a different cluster to compare the distribution of variable across different clusters\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "fig, axs = plt.subplots(nrows=4, ncols=3, figsize=(16, 18))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.boxplot([data[data['kmeans_cluster']==0][features[i]],\n",
    "                 data[data['kmeans_cluster']==1][features[i]],\n",
    "                 data[data['kmeans_cluster']==2][features[i]]])\n",
    "    ax.set_xticklabels(['Cluster 0', 'Cluster 1', 'Cluster 2'])\n",
    "    ax.set_ylabel(features[i])\n",
    "\n",
    "fig.suptitle('K-Means Clustering Results', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the Correlation heatmap\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(corr, cmap='coolwarm', annot=True, fmt='.2f', annot_kws={\"size\": 10}, \n",
    "            xticklabels=corr.columns, yticklabels=corr.columns)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "df5 = pd.read_excel('data111.xlsx', sheet_name='data')\n",
    "df = df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode 'Sex'\n",
    "le = LabelEncoder()\n",
    "df['Sex'] = le.fit_transform(df['Sex'])\n",
    "\n",
    "# create a function to categorize BMI into 4 categories\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return 'Underweight'\n",
    "    elif bmi >= 18.5 and bmi <= 24.9:\n",
    "        return 'Healthy Weight'\n",
    "    elif bmi >= 25.0 and bmi <= 29.9:\n",
    "        return 'Overweight'\n",
    "    else:\n",
    "        return 'Obesity'\n",
    "\n",
    "# apply the function to create a new column called BMI Category\n",
    "df['BMI Category'] = df['BMI'].apply(categorize_bmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the target variable and features\n",
    "target = 'BMI Category'\n",
    "features = ['SH', 'SUH', 'EH', 'AD', 'BPL', 'PH', 'KH', 'TH', 'HW', 'Age', 'Sex']\n",
    "n = 42\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=n)\n",
    "\n",
    "# preprocess the data by scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform feature selection using a decision tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=n)\n",
    "dt.fit(X_train, y_train)\n",
    "importance = dt.feature_importances_\n",
    "feature_importance = dict(zip(features, importance))\n",
    "important_features = sorted(feature_importance, key=feature_importance.get, reverse=True)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of important features in the original features list\n",
    "important_feature_indices = [features.index(f) for f in important_features]\n",
    "\n",
    "# select the important features for X_train and X_test\n",
    "X_train = X_train[:, important_feature_indices]\n",
    "X_test = X_test[:, important_feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and visualize feature importance plot\n",
    "sns.barplot(x=list(feature_importance.values()), y=list(feature_importance.keys()), orient='h')\n",
    "plt.title('Feature Importance Plot')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Features')\n",
    "print (list(feature_importance.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom scoring function for precision, recall, and F1-score\n",
    "scoring = {'accuracy': 'accuracy', 'precision': make_scorer(precision_score, average='weighted', zero_division=1),\n",
    "           'recall': make_scorer(recall_score, average='weighted', zero_division=1),\n",
    "           'f1_score': make_scorer(f1_score, average='weighted', zero_division=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate the decision tree classifier\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "dt_cv_results = cross_validate(dt, X_train, y_train, cv=5, scoring=scoring)\n",
    "print(\"Decision Tree Classifier Cross Validation Evaluation Scores:\")\n",
    "print(\"Accuracy:\", np.mean(dt_cv_results['test_accuracy']), ':', dt_cv_results['test_accuracy'])\n",
    "print(\"Precision:\", np.mean(dt_cv_results['test_precision']), ':', dt_cv_results['test_precision'])\n",
    "print(\"Recall:\", np.mean(dt_cv_results['test_recall']), ':', dt_cv_results['test_recall'])\n",
    "print(\"F1-Score:\", np.mean(dt_cv_results['test_f1_score']), ':', dt_cv_results['test_f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix for decision tree classifier\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Decision Tree Classifier Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "# train and evaluate the support vector machine classifier\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "svm_cv_results = cross_validate(svm, X_train, y_train, cv=5, scoring=scoring)\n",
    "print(\"Support Vector Machine Cross Validation Evaluation Scores:\")\n",
    "print(\"Accuracy:\", np.mean(svm_cv_results['test_accuracy']), ':', svm_cv_results['test_accuracy'])\n",
    "print(\"Precision:\", np.mean(svm_cv_results['test_precision']), ':', svm_cv_results['test_precision'])\n",
    "print(\"Recall:\", np.mean(svm_cv_results['test_recall']), ':', svm_cv_results['test_recall'])\n",
    "print(\"F1-Score:\", np.mean(svm_cv_results['test_f1_score']), ':', svm_cv_results['test_f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix for support vector machine classifier\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Support Vector Machine Classifier Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the logistic regression model\n",
    "lr = LogisticRegression(random_state=n)\n",
    "\n",
    "# train and evaluate the logistic regression model\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "lr_cv_results = cross_validate(lr, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "# print the evaluation scores\n",
    "print(\"Logistic Regression Cross Validation Evaluation Scores:\")\n",
    "print(\"Accuracy:\", np.mean(lr_cv_results['test_accuracy']), ':', lr_cv_results['test_accuracy'])\n",
    "print(\"Precision:\", np.mean(lr_cv_results['test_precision']), ':', lr_cv_results['test_precision'])\n",
    "print(\"Recall:\", np.mean(lr_cv_results['test_recall']), ':', lr_cv_results['test_recall'])\n",
    "print(\"F1-Score:\", np.mean(lr_cv_results['test_f1_score']), ':', lr_cv_results['test_f1_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix for logistic regression model\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Logistic Regression Classifier Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Association Rule Learning/Mining\n",
    "\n",
    "df = pd.read_excel('dataset11.xlsx', sheet_name='data')\n",
    "\n",
    "# create a function to categorize BMI into 4 categories\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return 'Underweight'\n",
    "    elif bmi >= 18.5 and bmi <= 24.9:\n",
    "        return 'Healthy Weight'\n",
    "    elif bmi >= 25.0 and bmi <= 29.9:\n",
    "        return 'Overweight'\n",
    "    else:\n",
    "        return 'Obesity'\n",
    "\n",
    "# apply the function to create a new column called BMI Category\n",
    "df['BMI Category'] = df['BMI'].apply(categorize_bmi)\n",
    "\n",
    "# create four new columns and set them to 1 or 0 based on BMI Category\n",
    "df['Underweight'] = df['BMI Category'].apply(lambda x: 1 if x == 'Underweight' else 0)\n",
    "df['Healthy Weight'] = df['BMI Category'].apply(lambda x: 1 if x == 'Healthy Weight' else 0)\n",
    "df['Overweight'] = df['BMI Category'].apply(lambda x: 1 if x == 'Overweight' else 0)\n",
    "df['Obesity'] = df['BMI Category'].apply(lambda x: 1 if x == 'Obesity' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the column names to calculate quartiles for\n",
    "columns_to_calculate_quartiles = ['SH', 'SUH', 'EH', 'AD', 'BPL', 'PH', 'KH', 'TH', 'HW']\n",
    "\n",
    "# loop through the list of column names and calculate quartiles for each column\n",
    "for col in columns_to_calculate_quartiles:\n",
    "    df[f'Q1 {col}'] = df[col].apply(lambda x: 1 if x <= df[col].quantile(0.25) else 0)\n",
    "    df[f'Q2 {col}'] = df[col].apply(lambda x: 1 if x > df[col].quantile(0.25) and x <= df[col].quantile(0.5) else 0)\n",
    "    df[f'Q3 {col}'] = df[col].apply(lambda x: 1 if x > df[col].quantile(0.5) and x <= df[col].quantile(0.75) else 0)\n",
    "    df[f'Q4 {col}'] = df[col].apply(lambda x: 1 if x > df[col].quantile(0.75) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the variables of interest\n",
    "vars_to_use = ['Underweight', 'Healthy Weight', 'Overweight', 'Obesity',\n",
    "                    'Q1 SH', 'Q1 SUH', 'Q1 EH', 'Q1 TH', 'Q1 AD', 'Q1 BPL', 'Q1 PH', 'Q1 KH', 'Q1 HW',\n",
    "                    'Q2 SH', 'Q2 SUH', 'Q2 EH', 'Q2 TH', 'Q2 AD', 'Q2 BPL', 'Q2 PH', 'Q2 KH', 'Q2 HW',\n",
    "                    'Q3 SH', 'Q3 SUH', 'Q3 EH', 'Q3 TH', 'Q3 AD', 'Q3 BPL', 'Q3 PH', 'Q3 KH', 'Q3 HW',\n",
    "                    'Q4 SH', 'Q4 SUH', 'Q4 EH', 'Q4 TH', 'Q4 AD', 'Q4 BPL', 'Q4 PH', 'Q4 KH', 'Q4 HW',\n",
    "                    'Neck', 'BSH', 'Back']\n",
    "df2 = df1[vars_to_use]\n",
    "\n",
    "\n",
    "# Generate frequent itemsets\n",
    "frequent_itemsets = apriori(df2, min_support=0.15, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_list = [tuple(rule['antecedents']) + tuple(rule['consequents']) for _, rule in rules.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 13\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "np.random.seed(r)  # to stop randomness\n",
    "\n",
    "# Create the graph\n",
    "G = nx.DiGraph()\n",
    "for rule in rule_list:\n",
    "    G.add_edge(rule[0], rule[1])\n",
    "\n",
    "np.random.seed(r)  # to stop randomness\n",
    "\n",
    "# Assign different colors to nodes based on their category\n",
    "color_map = {'Q1 SH': 'red', 'Q1 SUH': 'green', 'Q1 EH': 'blue', 'Q1 TH': 'purple', 'Q1 AD': 'orange', \n",
    "             'Q1 BPL': 'cyan', 'Q1 PH': 'magenta', 'Q1 KH': 'yellow', 'Q1 HW': 'brown', \n",
    "             'Q2 TH': 'gray', 'Q2 AD': 'indigo', \n",
    "             'Q2 BPL': 'olive', 'Q2 HW': 'darkgray', \n",
    "             'Q3 EH': 'peru', 'Q3 AD': 'darkorange', 'Q3 HW': 'saddlebrown', \n",
    "             'Underweight': 'darkslateblue', 'Overweight': 'limegreen',\n",
    "             'Healthy Weight': 'blueviolet'}\n",
    "\n",
    "np.random.seed(r)  # to stop randomness\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))  # increase the figure size for better visibility\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=2700, node_color=[color_map[n] for n in G.nodes()])\n",
    "nx.draw_networkx_edges(G, pos, width=1.5, arrowstyle='->', arrowsize=45)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_family=\"Times New Roman\")\n",
    "plt.axis(\"off\")\n",
    "plt.title('Association Rules Network Graph', fontsize=20)\n",
    "\n",
    "np.random.seed(r)  # to stop randomness\n",
    "\n",
    "# Add legend to the plot\n",
    "patches = [plt.plot([], [], marker=\"o\", ms=10, ls=\"\", mec=None, color=color_map[n], label=n)[0] for n in color_map.keys()]\n",
    "plt.legend(handles=patches, fontsize=10, loc='best')\n",
    "\n",
    "np.random.seed(r)  # to stop randomness\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
